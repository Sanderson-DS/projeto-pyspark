{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7c4e35",
   "metadata": {},
   "source": [
    "# Iniciando uma sessão Spark para testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf63317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: C:\\Java\\jdk-25.0.2\n",
      "SPARK_HOME: C:\\Spark\n",
      "HADOOP_HOME: C:\\hadoop\n",
      "Spark version: 4.1.1\n",
      "Spark UI: http://127.0.0.1:4040\n",
      "spark.sql.warehouse.dir: C:\\tmp\\hive\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.makedirs(r\"C:\\tmp\\spark\", exist_ok=True)\n",
    "os.makedirs(r\"C:\\tmp\\hive\", exist_ok=True)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"UI-Stable\")\n",
    "    # UI/host\n",
    "    .config(\"spark.ui.enabled\", \"true\")\n",
    "    .config(\"spark.ui.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    # dirs/temp\n",
    "    .config(\"spark.local.dir\", r\"C:\\tmp\\spark\")\n",
    "    .config(\"spark.sql.warehouse.dir\", r\"C:\\tmp\\hive\")\n",
    "    # performance/estabilidade (menos memória)\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "    .config(\"spark.default.parallelism\", \"16\")\n",
    "    # limites de memória (evita estourar)\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"JAVA_HOME:\", os.getenv(\"JAVA_HOME\"))\n",
    "print(\"SPARK_HOME:\", os.getenv(\"SPARK_HOME\"))\n",
    "print(\"HADOOP_HOME:\", os.getenv(\"HADOOP_HOME\"))\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"spark.sql.warehouse.dir:\", spark.conf.get(\"spark.sql.warehouse.dir\"))\n",
    "\n",
    "\n",
    "spark.range(10).show()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977217da",
   "metadata": {},
   "source": [
    "# verificação do Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306c47e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uiWebUrl: http://127.0.0.1:4040\n",
      "spark.ui.enabled: true\n",
      "spark.ui.port: 4040\n"
     ]
    }
   ],
   "source": [
    "print(\"uiWebUrl:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"spark.ui.enabled:\", spark.conf.get(\"spark.ui.enabled\", \"true\"))\n",
    "print(\"spark.ui.port:\", spark.conf.get(\"spark.ui.port\", \"4040\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ac55c",
   "metadata": {},
   "source": [
    "# rodar um job “demorado” pra aparecer Jobs/Stages na UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b928be19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(0, 20_000_000).repartition(16)\n",
    "df.groupBy((df.id % 1000).alias(\"k\")).count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40507b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.range(0, 30_000_000).withColumn(\"k\", (F.col(\"id\") % 10000))\n",
    "df = df.repartition(16, \"k\")\n",
    "res = df.groupBy(\"k\").count()\n",
    "res.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ee8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (venv)",
   "language": "python",
   "name": "pyspark-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
